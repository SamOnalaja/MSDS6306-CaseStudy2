---
title: "Case Study 2: Employee Data Analysis"
author: "Meredith Ludlow & Kristen Rollins"
date: "December 9, 2018"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(knitr)
library(dplyr)
library(caret)
library(kableExtra)
```

# Summary

TODO 

# Introduction

The purpose of this analysis is to explore what variables are good predictors for attrition rates in Fortune 1000 companies. Exploratory analytics will be used to determine which variables are the best predictors of attrition, as well as looking at other trends associated with specific jobs. Finally, we will create a model that will predict whether or not an employee will leave the company voluntarily.

# Analysis

### Exploratory Data Analysis

```{r}
# Read in training data
dfTrain <- read.csv("CaseStudy2-data.csv")
```

If a variable does not have a significant impact on turnover, we would expect that the attrition rate within a group is the same as the attrition rate of the entire dataset. As we see below, in the whole training set 83.9% of employees stayed while 16.1% left. So, as we view the relative rates for turnover for each categorical variable, we would expect variables with high attrition rates to be strong predictors of turnover. 

We excluded a few variables from consideration at the start because they had the same value for every employee and wouldn't be any use as predictors. Then, we took all of the numerical values and broke them up into different levels based on ranges that we chose.

```{r}
# Percentage of retained/lost employees
kable(table(dfTrain$Attrition) / nrow(dfTrain), 
      col.names=c("Attrition", "Percent")) %>% 
      kable_styling(full_width=FALSE)

# Define variables for analysis
variables <- c("BusinessTravel", "Department", "Education", 
              "EducationField", "EnvironmentSatisfaction",
              "Gender", "JobInvolvement", "JobLevel",
              "JobRole", "JobSatisfaction", "MaritalStatus",
              "OverTime", "PerformanceRating", 
              "RelationshipSatisfaction", "StockOptionLevel", 
              "WorkLifeBalance", "Age", "DailyRate", "DistanceFromHome", 
              "HourlyRate", "MonthlyIncome", "MonthlyRate", 
              "NumCompaniesWorked", "PercentSalaryHike",
              "TotalWorkingYears", "TrainingTimesLastYear", 
              "YearsAtCompany", "YearsInCurrentRole", 
              "YearsSinceLastPromotion", "YearsWithCurrManager")

# Turn numerical values into categorical
dfTrain$Age <- cut(dfTrain$Age, breaks=c(-Inf,30,40,50,Inf),labels=c(1,2,3,4))
dfTrain$DailyRate <- cut(dfTrain$DailyRate, breaks=c(-Inf,500,1000,Inf),labels=c(1,2,3))
dfTrain$DistanceFromHome <- cut(dfTrain$DistanceFromHome, breaks=c(-Inf,10,Inf),labels=c(1,2))
dfTrain$HourlyRate <- cut(dfTrain$HourlyRate, breaks=c(-Inf,65,Inf),labels=c(1,2))
dfTrain$MonthlyIncome <- cut(dfTrain$MonthlyIncome, breaks=c(-Inf,5000,10000,15000,Inf),labels=c(1,2,3,4))
dfTrain$MonthlyRate <- cut(dfTrain$MonthlyRate, breaks=c(-Inf,5000,10000,15000,20000,Inf),labels=c(1,2,3,4,5))
dfTrain$NumCompaniesWorked <- cut(dfTrain$NumCompaniesWorked, breaks=c(-Inf,4,Inf),labels=c(1,2))
dfTrain$PercentSalaryHike <- cut(dfTrain$PercentSalaryHike, breaks=c(-Inf, 13, 17, Inf), labels=c(1, 2, 3))
dfTrain$TotalWorkingYears <- cut(dfTrain$TotalWorkingYears, breaks=c(-Inf, 10, 20, Inf), labels=c(1, 2, 3))
dfTrain$TrainingTimesLastYear <- cut(dfTrain$TrainingTimesLastYear, breaks=c(-Inf, 1, 4, Inf), labels=c(1, 2, 3))
dfTrain$YearsAtCompany <- cut(dfTrain$YearsAtCompany, breaks=c(-Inf, 5, 15, 25, Inf), labels=c(1, 2, 3, 4))
dfTrain$YearsInCurrentRole <- cut(dfTrain$YearsInCurrentRole, breaks=c(-Inf, 5, 10, Inf), labels=c(1, 2, 3))
dfTrain$YearsSinceLastPromotion <- cut(dfTrain$YearsSinceLastPromotion, breaks=c(-Inf, 2, 7, Inf), labels=c(1, 2, 3))
dfTrain$YearsWithCurrManager <- cut(dfTrain$YearsWithCurrManager, breaks=c(-Inf, 5, 10, Inf), labels=c(1, 2, 3))
```

The tables generated below are the attrition rate for each level within every variable.

```{r results="asis"}
# Make relative frequency tables for categorical variables
AbsDiff <- data.frame(Variable=character(), AvgDistance=numeric())
for (var in variables) {
  print(var)
  freqtable <- table(dfTrain[[var]], dfTrain$Attrition)
  count <- plyr::count(dfTrain[[var]])
  RelFreq <- freqtable / count$freq
  print(kable(RelFreq,row.names=TRUE) %>% kable_styling(full_width=FALSE))
  Sum <- sum(abs(RelFreq[,2]-0.1606838))/nrow(RelFreq)
  AbsDiff <- rbind(AbsDiff, data.frame(Variable=var, AverageDistance=Sum))
}
```

In order to figure out which variables had attrition rates that were the most different from the attirtion rate of the data set as a whole, we had to create a metric. The metric that we used was the average absolute difference in attrition rates. We took the attrition rates under a variable and found the average difference between them and the total attrition rate. The varaibles with the higest average difference are listed in the table below.

```{r}
# Get average distance metric for all variables
AbsDiff <- AbsDiff[order(-AbsDiff$AverageDistance),]
kable(AbsDiff,row.names=FALSE) %>% kable_styling(full_width=FALSE)
```

### KNN Classification

We used a KNN in model to predict whether or not an employee will leave for employees in a new data set. We used the top 6 variables in the above table as our predictors in the model. We ran the model looking at the three closest points and the five closest points to see which gave better results.

```{r}
# Read in validation data
dfVal <- read.csv("CaseStudy2Validation.csv")

# Identify variables used to make predictions, based on avg dist metric
pred_vars <- c("OverTime", "JobRole", "JobInvolvement", "JobLevel", "MaritalStatus","WorkLifeBalance")

# Convert wanted factors into integers
dfTrain$OverTime <- as.integer(dfTrain$OverTime)
dfTrain$JobRole <- as.integer(dfTrain$JobRole)
dfTrain$MaritalStatus <- as.integer(dfTrain$MaritalStatus)
dfVal$OverTime <- as.integer(dfVal$OverTime)
dfVal$JobRole <- as.integer(dfVal$JobRole)
dfVal$MaritalStatus <- as.integer(dfVal$MaritalStatus)

# Generate attrition predictions based on training data
dfVal$dfPreds3 <- class::knn(dfTrain[,pred_vars], dfVal[,pred_vars], 
                             dfTrain$Attrition, k=3)
dfVal$dfPreds5 <- class::knn(dfTrain[,pred_vars], dfVal[,pred_vars], 
                             dfTrain$Attrition, k=5)

# Get accuracy of predictions
confusionMatrix(table(dfVal$Attrition, dfVal$dfPreds3))
confusionMatrix(table(dfVal$Attrition, dfVal$dfPreds5))

dfPreds <- select(dfVal, ID, dfPreds3)

# Write predictions to csv file
# write.csv(dfPreds, "CaseStudy2Predictions_Ludlow_Rollins.csv")
```

The KNN model that looks at the three closest data points to the test data point has a higher accuracy than the model that looks at 5. The accuracy is 85.67%. Our model is able to predict whether or not an employee will leave 85.67% of the time. The sensitivity of the model is 86.36% meaning that 86.36% of the time when the model labeled a person as not leaving, it was correct. The specificity of the model was 71.43% meaning that when someone did leave, the model was able to predict it 71.43% of the time. 

### Employee Trends

```{r jobrole_jobsat, fig.align='center'}
# Re-load original data
dfTrain <- read.csv("CaseStudy2-data.csv")
# Job satisfaction by group
Jobs <- group_by(dfTrain, JobRole) %>% summarise(Avg=mean(JobSatisfaction, na.rm=TRUE))
print(Jobs)
ggplot(data=Jobs, aes(x=JobRole, y=Avg, fill=JobRole)) + geom_bar(stat='identity', colour = 'black') + coord_flip() + ggtitle("Average Job Satisfaction by Job") + xlab("Job Type") + ylab("Average Satisfaction") + theme(legend.position="none") + theme(plot.title = element_text(hjust = 0.5))
ggplot(data=Jobs, aes(x=JobRole, y=Avg, fill=JobRole)) + 
  geom_bar(stat='identity', colour = 'black') + 
  coord_flip() + 
  labs(title="Mean Job Satisfaction by Role", x="Job Role", y="Mean Job Satisfaction") +
  theme(legend.position="none") + 
  theme(plot.title = element_text(hjust = 0.5))
```

The average job satisfaction for each position is fairly close together. The lowest is Human Resources at 2.57 and the highest is Research Scientist and Healthcare Representative at 2.80. 


```{r income_jobsat}
# Scatterplot of Monthly Income by Job Role
ggplot(dfTrain, aes(x=JobRole, y=MonthlyIncome)) + ggtitle("Income Distribution by Job Type") + xlab("Job Type") + ylab("Monthly Income") + 
  geom_point(pch = 21, size = 3, color="blue") + theme(axis.text.x = element_text(angle = 50, hjust = 1))

# Scatterplot of Age by Job Role
ggplot(dfTrain, aes(x=JobRole, y=Age)) + ggtitle("Age Distribution by Job Type") + xlab("Job Type") + ylab("Age") + 
  geom_point(pch = 21, size = 3, color="red") + theme(axis.text.x = element_text(angle = 50, hjust = 1))

# Attrition rate by Job Role
freqtable <- table(dfTrain$JobRole, dfTrain$Attrition)
count <- plyr::count(dfTrain$JobRole)
RelFreq <- freqtable / count$freq
dfRel <- data.frame(RelFreq)
dfRel2 <- dfRel[10:18,]
ggplot(data=dfRel2, aes(x=Var1, y=Freq, fill=Var1)) + geom_bar(stat='identity', colour = 'black') + coord_flip() + ggtitle("Attrition Rate per Job Type") + ylab("Attrition Rate") + xlab("Job Type") + theme(legend.position="none") + theme(plot.title = element_text(hjust = 0.5))

ggplot(dfTrain, aes(x=JobSatisfaction, y=MonthlyIncome)) + 
  geom_point(pch = 21, size = 2, color="green") +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(dfTrain, aes(x=JobSatisfaction, y=MonthlyIncome, group=JobSatisfaction)) +
  geom_boxplot() +
  stat_summary(fun.y=mean, geom="point", colour="lightgreen") +
  theme(plot.title = element_text(hjust = 0.5))

```



# Conclusion

TODO

# Presentation

This write-up is supplemented by video presentations from both Meredith and Kristen. The links are provided below.

Meredith: 

Kristen: 
